{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# Brianne Bell \n",
    "## ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics\n",
    "### January 23, 2023\n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import textacy.preprocessing as tprep\n",
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter/\n",
      "lyrics/\n"
     ]
    }
   ],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:/Users/breel.B-E-BELL/OneDrive/Documents/GitHub/ADS-509_M2-textmine-token-norm/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "#twitter_folder = \"C:/Users/breel.B-E-BELL/OneDrive/Documents/GitHub/ADS-509_M2-textmine-token-norm/twitter\"\n",
    "#lyrics_folder = \"C:/Users/breel.B-E-BELL/OneDrive/Documents/GitHub/ADS-509_M2-textmine-token-norm/lyrics\"\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "print(twitter_folder)\n",
    "print(lyrics_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens/num_tokens # unique/all\n",
    "    num_characters = sum(len(i) for i in tokens)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(\"\\nThe top 5 most common tokens:\\n\") # {name} gives updated value\n",
    "        print(Counter(tokens).most_common(5))\n",
    "        \n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "\n",
      "The top 5 most common tokens:\n",
      "\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "---\n",
    "**Q:** Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "**A:** Assertion statements let you check that your code is still true which indicates that your code is not buggy (at that point in the code). This will allow programming to run more smoothly because you won't have to hunt down a bug because you will know where it has appeared. \n",
    "\n",
    "https://realpython.com/python-assert-statement/#:~:text=Python%E2%80%99s%20assert%20statement%20allows%20you%20to%20write%20sanity,then%20you%20have%20a%20bug%20in%20your%20code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a6dd663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cher', 'robyn']\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/breel.B-E-BELL/OneDrive/Documents/GitHub/ADS-509_M2-textmine-token-norm/lyrics\"\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "37d70801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>song_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"88 Degrees\"\\n</td>\n",
       "      <td>\"88 Degrees\"\\n\\n\\n\\nStuck in L.A., ain't got n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"A Different Kind Of Love Song\"\\n</td>\n",
       "      <td>\"A Different Kind Of Love Song\"\\n\\n\\n\\nWhat if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"After All\"\\n</td>\n",
       "      <td>\"After All\"\\n\\n\\n\\nWell, here we are again\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Again\"\\n</td>\n",
       "      <td>\"Again\"\\n\\n\\n\\nAgain evening finds me at your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Alfie\"\\n</td>\n",
       "      <td>\"Alfie\"\\n\\n\\n\\nWhat's it all about, Alfie?\\nIs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                         song_title  \\\n",
       "0   cher                     \"88 Degrees\"\\n   \n",
       "1   cher  \"A Different Kind Of Love Song\"\\n   \n",
       "2   cher                      \"After All\"\\n   \n",
       "3   cher                          \"Again\"\\n   \n",
       "4   cher                          \"Alfie\"\\n   \n",
       "\n",
       "                                         song_lyrics  \n",
       "0  \"88 Degrees\"\\n\\n\\n\\nStuck in L.A., ain't got n...  \n",
       "1  \"A Different Kind Of Love Song\"\\n\\n\\n\\nWhat if...  \n",
       "2  \"After All\"\\n\\n\\n\\nWell, here we are again\\nI ...  \n",
       "3  \"Again\"\\n\\n\\n\\nAgain evening finds me at your ...  \n",
       "4  \"Alfie\"\\n\\n\\n\\nWhat's it all about, Alfie?\\nIs...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "#initializing lists\n",
    "artists = []\n",
    "song_title = []\n",
    "song_lyrics = []\n",
    "\n",
    "lyrics_data = {}\n",
    "\n",
    "# looping through the files\n",
    "for artist in os.listdir(path):\n",
    "    artist_name = os.path.join(path, artist)\n",
    "    \n",
    "    for file in os.listdir(artist_name):\n",
    "        filename = os.path.join(path, artist, file)\n",
    "        \n",
    "        with open(filename) as infile:\n",
    "            songs = infile.readlines()\n",
    "            \n",
    "        artists.append(artist)\n",
    "        song_title.append(songs[0])\n",
    "        song_lyrics.append(''.join(songs))\n",
    "        \n",
    "        \n",
    "lyrics_df = pd.DataFrame({'artist': artists,\n",
    "                         'song_title': song_title,\n",
    "                         'song_lyrics':song_lyrics})\n",
    "\n",
    "#checking\n",
    "lyrics_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9482e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>song_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"We Dance To The Beat\"\\n</td>\n",
       "      <td>\"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Who's That Girl\"\\n</td>\n",
       "      <td>\"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"With Every Heartbeat\"\\n</td>\n",
       "      <td>\"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"You've Got That Something\"\\n</td>\n",
       "      <td>\"You've Got That Something\"\\n\\n\\n\\nLook at me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                     song_title  \\\n",
       "415  robyn       \"We Dance To The Beat\"\\n   \n",
       "416  robyn      \"Where Did Our Love Go\"\\n   \n",
       "417  robyn            \"Who's That Girl\"\\n   \n",
       "418  robyn       \"With Every Heartbeat\"\\n   \n",
       "419  robyn  \"You've Got That Something\"\\n   \n",
       "\n",
       "                                           song_lyrics  \n",
       "415  \"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...  \n",
       "416  \"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...  \n",
       "417  \"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...  \n",
       "418  \"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...  \n",
       "419  \"You've Got That Something\"\\n\\n\\n\\nLook at me ...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking tail\n",
    "lyrics_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec5d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up path to read in \n",
    "twitter_folder2 = \"C:/Users/breel.B-E-BELL/OneDrive/Documents/GitHub/ADS-509_M2-textmine-token-norm/twitter\"\n",
    "twitter_descrip_dict = defaultdict(list)\n",
    "\n",
    "for path in os.listdir(twitter_folder2):\n",
    "    artist_folder_path = os.path.join(twitter_folder2, path)\n",
    "    openfile_to_read = open(artist_folder_path, \"r\", encoding= 'utf-8', errors= 'ignore')\n",
    "    for line in openfile_to_read.readlines():\n",
    "        current_line = (line.split('\\t'))\n",
    "        if(len(current_line) >= 6):\n",
    "            description = current_line[6]\n",
    "            description = description.strip()\n",
    "            if description != 'description' and description != '':\n",
    "                twitter_descrip_dict[path.split('_')[0]].append(description)\n",
    "\n",
    "\n",
    "# chekcing            \n",
    "# twitter_descrip_dict\n",
    "\n",
    "twitter_descrip_df = pd.DataFrame(dict([(k, pd.Series(v)) for k,v in twitter_descrip_dict.items()]))\n",
    "#twitter_descrip_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30f79db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1297966</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158251</th>\n",
       "      <td>cher</td>\n",
       "      <td>IFFBoston Volunteer Coordinator, Cyclist, Movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556595</th>\n",
       "      <td>cher</td>\n",
       "      <td>DjBojack: Tallahassee Own / Promoter &amp; Promoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626810</th>\n",
       "      <td>cher</td>\n",
       "      <td>British, pro-business, pro-common sense &amp; pro-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969198</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934371</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818472</th>\n",
       "      <td>cher</td>\n",
       "      <td>Follow my nigga @lil bibby Follow me I follow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854589</th>\n",
       "      <td>cher</td>\n",
       "      <td>I just wanna explore the world with my eyes in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968622</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443233</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                                        description\n",
       "1297966  robynkonichiwa                                                nan\n",
       "1158251            cher  IFFBoston Volunteer Coordinator, Cyclist, Movi...\n",
       "1556595            cher  DjBojack: Tallahassee Own / Promoter & Promoti...\n",
       "626810             cher  British, pro-business, pro-common sense & pro-...\n",
       "969198   robynkonichiwa                                                nan\n",
       "1934371  robynkonichiwa                                                nan\n",
       "1818472            cher  Follow my nigga @lil bibby Follow me I follow ...\n",
       "854589             cher  I just wanna explore the world with my eyes in...\n",
       "968622   robynkonichiwa                                                nan\n",
       "1443233  robynkonichiwa                                                nan"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating out\n",
    "cher_twitter = pd.DataFrame({'artist': 'cher', 'description': twitter_descrip_df['cher']})\n",
    "#cher_twitter.head(5)\n",
    "robyn_twitter = pd.DataFrame({'artist': 'robynkonichiwa', 'description': twitter_descrip_df['robynkonichiwa']})\n",
    "robyn_twitter.head(5)\n",
    "\n",
    "# combining for cleaning:\n",
    "twitter_df = pd.concat([cher_twitter, robyn_twitter])\n",
    "twitter_df['description'] = twitter_df['description'].astype(str)\n",
    "#checking\n",
    "twitter_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "#punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b327033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for cleaning\n",
    "\n",
    "# Removing punctuation\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([i for i in text if i not in punctuation])\n",
    "\n",
    "# splitting on whitespace\n",
    "def split_on_whitespace(text):\n",
    "    #text= text.str.split()\n",
    "    text= re.sub(r'\\s+', ' ', text) \n",
    "    return text.strip()\n",
    "\n",
    "# tokenization function\n",
    "def tokenization(text):\n",
    "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)\n",
    "\n",
    "# remove stopwords (sw)\n",
    "def remove_stopwords(tokens):\n",
    "    text_list= [i for i in tokens if i.lower() not in sw] # gives back i if it's not a stopword\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69c38fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>no_ws</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1628802</th>\n",
       "      <td>cher</td>\n",
       "      <td>singer/songwriter</td>\n",
       "      <td>singersongwriter</td>\n",
       "      <td>singersongwriter</td>\n",
       "      <td>[singersongwriter]</td>\n",
       "      <td>[singersongwriter]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177507</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814382</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468803</th>\n",
       "      <td>cher</td>\n",
       "      <td>Comunicadora social</td>\n",
       "      <td>Comunicadora social</td>\n",
       "      <td>comunicadora social</td>\n",
       "      <td>[comunicadora, social]</td>\n",
       "      <td>[comunicadora, social]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989886</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977152</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915657</th>\n",
       "      <td>cher</td>\n",
       "      <td>My. lawyer. liked. that.</td>\n",
       "      <td>My lawyer liked that</td>\n",
       "      <td>my lawyer liked that</td>\n",
       "      <td>[my, lawyer, liked, that]</td>\n",
       "      <td>[lawyer, liked]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595692</th>\n",
       "      <td>cher</td>\n",
       "      <td>üå∏üå∫</td>\n",
       "      <td>üå∏üå∫</td>\n",
       "      <td>üå∏üå∫</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171631</th>\n",
       "      <td>cher</td>\n",
       "      <td>I'm a security guard at casting solutions</td>\n",
       "      <td>Im a security guard at casting solutions</td>\n",
       "      <td>im a security guard at casting solutions</td>\n",
       "      <td>[im, a, security, guard, at, casting, solutions]</td>\n",
       "      <td>[im, security, guard, casting, solutions]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574118</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                                description  \\\n",
       "1628802            cher                          singer/songwriter   \n",
       "1177507  robynkonichiwa                                        nan   \n",
       "814382   robynkonichiwa                                        nan   \n",
       "1468803            cher                        Comunicadora social   \n",
       "1989886  robynkonichiwa                                        nan   \n",
       "1977152  robynkonichiwa                                        nan   \n",
       "915657             cher                   My. lawyer. liked. that.   \n",
       "1595692            cher                                         üå∏üå∫   \n",
       "171631             cher  I'm a security guard at casting solutions   \n",
       "1574118  robynkonichiwa                                        nan   \n",
       "\n",
       "                                          no_punc  \\\n",
       "1628802                          singersongwriter   \n",
       "1177507                                       nan   \n",
       "814382                                        nan   \n",
       "1468803                       Comunicadora social   \n",
       "1989886                                       nan   \n",
       "1977152                                       nan   \n",
       "915657                       My lawyer liked that   \n",
       "1595692                                        üå∏üå∫   \n",
       "171631   Im a security guard at casting solutions   \n",
       "1574118                                       nan   \n",
       "\n",
       "                                            no_ws  \\\n",
       "1628802                          singersongwriter   \n",
       "1177507                                       nan   \n",
       "814382                                        nan   \n",
       "1468803                       comunicadora social   \n",
       "1989886                                       nan   \n",
       "1977152                                       nan   \n",
       "915657                       my lawyer liked that   \n",
       "1595692                                        üå∏üå∫   \n",
       "171631   im a security guard at casting solutions   \n",
       "1574118                                       nan   \n",
       "\n",
       "                                                tokenized  \\\n",
       "1628802                                [singersongwriter]   \n",
       "1177507                                             [nan]   \n",
       "814382                                              [nan]   \n",
       "1468803                            [comunicadora, social]   \n",
       "1989886                                             [nan]   \n",
       "1977152                                             [nan]   \n",
       "915657                          [my, lawyer, liked, that]   \n",
       "1595692                                                []   \n",
       "171631   [im, a, security, guard, at, casting, solutions]   \n",
       "1574118                                             [nan]   \n",
       "\n",
       "                                            tokens hashtags  \n",
       "1628802                         [singersongwriter]       []  \n",
       "1177507                                      [nan]       []  \n",
       "814382                                       [nan]       []  \n",
       "1468803                     [comunicadora, social]       []  \n",
       "1989886                                      [nan]       []  \n",
       "1977152                                      [nan]       []  \n",
       "915657                             [lawyer, liked]       []  \n",
       "1595692                                         []       []  \n",
       "171631   [im, security, guard, casting, solutions]       []  \n",
       "1574118                                      [nan]       []  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean twitter data here\n",
    "\n",
    "# remove punctuation\n",
    "twitter_df['no_punc'] = twitter_df['description'].map(remove_punctuation)\n",
    "\n",
    "# remove whitespace\n",
    "twitter_df['no_ws'] = twitter_df['no_punc'].map(split_on_whitespace)\n",
    "\n",
    "# making lowercase\n",
    "twitter_df['no_ws'] = twitter_df['no_ws'].apply(lambda x: x.lower())\n",
    "\n",
    "# tokenizing\n",
    "twitter_df['tokenized'] = twitter_df['no_ws'].map(tokenization)\n",
    "\n",
    "# removing stop words\n",
    "twitter_df['tokens'] = twitter_df['tokenized'].map(remove_stopwords)\n",
    "\n",
    "twitter_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>song_lyrics</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"It Might As Well Stay Monday (From Now On)\"\\n</td>\n",
       "      <td>it might as well stay monday from now on here ...</td>\n",
       "      <td>[it, might, as, well, stay, monday, from, now,...</td>\n",
       "      <td>[might, well, stay, monday, monday, morning, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"By Myself\"\\n</td>\n",
       "      <td>by myself i gotta go on my way by myself becau...</td>\n",
       "      <td>[by, myself, i, gotta, go, on, my, way, by, my...</td>\n",
       "      <td>[gotta, go, way, end, romance, gotta, go, way,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"O Baby\"\\n</td>\n",
       "      <td>o baby chorus o baby youre making that mistake...</td>\n",
       "      <td>[o, baby, chorus, o, baby, youre, making, that...</td>\n",
       "      <td>[baby, chorus, baby, youre, making, mistake, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Monday Morning\"\\n</td>\n",
       "      <td>monday morning whispered friday saturday sunda...</td>\n",
       "      <td>[monday, morning, whispered, friday, saturday,...</td>\n",
       "      <td>[monday, morning, whispered, friday, saturday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"One Of Us\"\\n</td>\n",
       "      <td>one of us they passed me by all of those great...</td>\n",
       "      <td>[one, of, us, they, passed, me, by, all, of, t...</td>\n",
       "      <td>[one, us, passed, great, romances, felt, robbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"We Dance To The Beat\"\\n</td>\n",
       "      <td>we dance to the beat we dance to the beat we d...</td>\n",
       "      <td>[we, dance, to, the, beat, we, dance, to, the,...</td>\n",
       "      <td>[dance, beat, dance, beat, dance, beat, dance,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Take Me For A Little While\"\\n</td>\n",
       "      <td>take me for a little while ive been trying to ...</td>\n",
       "      <td>[take, me, for, a, little, while, ive, been, t...</td>\n",
       "      <td>[take, little, ive, trying, make, love, everyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"A Young Girl (Une Enfante)\"\\n</td>\n",
       "      <td>a young girl une enfante she left her neighbor...</td>\n",
       "      <td>[a, young, girl, une, enfante, she, left, her,...</td>\n",
       "      <td>[young, girl, une, enfante, left, neighborhood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"In My Heart\"\\n</td>\n",
       "      <td>in my heart hope things will get better cause ...</td>\n",
       "      <td>[in, my, heart, hope, things, will, get, bette...</td>\n",
       "      <td>[heart, hope, things, get, better, cause, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Walk With Me\"\\n</td>\n",
       "      <td>walk with me standing all alone inside a crowd...</td>\n",
       "      <td>[walk, with, me, standing, all, alone, inside,...</td>\n",
       "      <td>[walk, standing, alone, inside, crowd, someone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                                      song_title  \\\n",
       "127   cher  \"It Might As Well Stay Monday (From Now On)\"\\n   \n",
       "27    cher                                   \"By Myself\"\\n   \n",
       "393  robyn                                      \"O Baby\"\\n   \n",
       "385  robyn                              \"Monday Morning\"\\n   \n",
       "191   cher                                   \"One Of Us\"\\n   \n",
       "414  robyn                        \"We Dance To The Beat\"\\n   \n",
       "241   cher                  \"Take Me For A Little While\"\\n   \n",
       "14    cher                  \"A Young Girl (Une Enfante)\"\\n   \n",
       "371  robyn                                 \"In My Heart\"\\n   \n",
       "281   cher                                \"Walk With Me\"\\n   \n",
       "\n",
       "                                           song_lyrics  \\\n",
       "127  it might as well stay monday from now on here ...   \n",
       "27   by myself i gotta go on my way by myself becau...   \n",
       "393  o baby chorus o baby youre making that mistake...   \n",
       "385  monday morning whispered friday saturday sunda...   \n",
       "191  one of us they passed me by all of those great...   \n",
       "414  we dance to the beat we dance to the beat we d...   \n",
       "241  take me for a little while ive been trying to ...   \n",
       "14   a young girl une enfante she left her neighbor...   \n",
       "371  in my heart hope things will get better cause ...   \n",
       "281  walk with me standing all alone inside a crowd...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "127  [it, might, as, well, stay, monday, from, now,...   \n",
       "27   [by, myself, i, gotta, go, on, my, way, by, my...   \n",
       "393  [o, baby, chorus, o, baby, youre, making, that...   \n",
       "385  [monday, morning, whispered, friday, saturday,...   \n",
       "191  [one, of, us, they, passed, me, by, all, of, t...   \n",
       "414  [we, dance, to, the, beat, we, dance, to, the,...   \n",
       "241  [take, me, for, a, little, while, ive, been, t...   \n",
       "14   [a, young, girl, une, enfante, she, left, her,...   \n",
       "371  [in, my, heart, hope, things, will, get, bette...   \n",
       "281  [walk, with, me, standing, all, alone, inside,...   \n",
       "\n",
       "                                                tokens  \n",
       "127  [might, well, stay, monday, monday, morning, r...  \n",
       "27   [gotta, go, way, end, romance, gotta, go, way,...  \n",
       "393  [baby, chorus, baby, youre, making, mistake, o...  \n",
       "385  [monday, morning, whispered, friday, saturday,...  \n",
       "191  [one, us, passed, great, romances, felt, robbi...  \n",
       "414  [dance, beat, dance, beat, dance, beat, dance,...  \n",
       "241  [take, little, ive, trying, make, love, everyt...  \n",
       "14   [young, girl, une, enfante, left, neighborhood...  \n",
       "371  [heart, hope, things, get, better, cause, that...  \n",
       "281  [walk, standing, alone, inside, crowd, someone...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean lyrics data here lyrics_df\n",
    "\n",
    "# remove punctuation\n",
    "lyrics_df['song_lyrics'] = lyrics_df['song_lyrics'].map(remove_punctuation)\n",
    "\n",
    "# remove whitespace\n",
    "lyrics_df['song_lyrics'] = lyrics_df['song_lyrics'].map(split_on_whitespace)\n",
    "\n",
    "# making lowercase\n",
    "lyrics_df['song_lyrics'] = lyrics_df['song_lyrics'].apply(lambda x: x.lower())\n",
    "\n",
    "# tokenizing\n",
    "lyrics_df['tokenized'] = lyrics_df['song_lyrics'].map(tokenization)\n",
    "\n",
    "# removing stop words\n",
    "lyrics_df['tokens'] = lyrics_df['tokenized'].map(remove_stopwords)\n",
    "\n",
    "lyrics_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for calls to descriptive_stats here\n",
    "\n",
    "# function to flatten tokens columns to list of lists for function\n",
    "    # then send it to the descriptive_stats(tokens, num_tokens = 5, verbose=True) function\n",
    "def flattened_descriptive_stats(list_of_lists):\n",
    "    listed_words = [i for n in list_of_lists for i in n]\n",
    "    return descriptive_stats(listed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac655544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cher Twitter Descriptive Stats:\n",
      "\n",
      "There are 14779802 tokens in the data.\n",
      "There are 1194810 unique tokens in the data.\n",
      "There are 89837791 characters in the data.\n",
      "The lexical diversity is 0.081 in the data.\n",
      "\n",
      "The top 5 most common tokens:\n",
      "\n",
      "[('love', 217449), ('im', 139831), ('life', 126418), ('music', 90024), ('de', 73084)]\n",
      "\n",
      "Cher Lyrics Descriptive Stats:\n",
      "\n",
      "There are 35911 tokens in the data.\n",
      "There are 3696 unique tokens in the data.\n",
      "There are 172596 characters in the data.\n",
      "The lexical diversity is 0.103 in the data.\n",
      "\n",
      "The top 5 most common tokens:\n",
      "\n",
      "[('love', 1004), ('im', 513), ('know', 486), ('dont', 440), ('youre', 333)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[35911, 3696, 0.102921110523238, 172596]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here <flattened_descriptive_stats>\n",
    "print('\\nCher Twitter Descriptive Stats:\\n')\n",
    "flattened_descriptive_stats(twitter_df[twitter_df['artist']=='cher']['tokens'])\n",
    "\n",
    "print('\\nCher Lyrics Descriptive Stats:\\n')\n",
    "flattened_descriptive_stats(lyrics_df[lyrics_df['artist']=='cher']['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5aab1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Robyn Twitter Descriptive Stats:\n",
      "\n",
      "There are 3236300 tokens in the data.\n",
      "There are 226027 unique tokens in the data.\n",
      "There are 14328650 characters in the data.\n",
      "The lexical diversity is 0.070 in the data.\n",
      "\n",
      "The top 5 most common tokens:\n",
      "\n",
      "[('nan', 1811066), ('music', 15345), ('love', 11820), ('im', 9096), ('och', 7923)]\n",
      "\n",
      "Robyn Lyrics Descriptive Stats:\n",
      "\n",
      "There are 15306 tokens in the data.\n",
      "There are 2156 unique tokens in the data.\n",
      "There are 73754 characters in the data.\n",
      "The lexical diversity is 0.141 in the data.\n",
      "\n",
      "The top 5 most common tokens:\n",
      "\n",
      "[('know', 308), ('dont', 301), ('im', 299), ('love', 275), ('got', 252)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15306, 2156, 0.14085979354501502, 73754]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here <flattened_descriptive_stats>\n",
    "print('\\nRobyn Twitter Descriptive Stats:\\n')\n",
    "flattened_descriptive_stats(twitter_df[twitter_df['artist']=='robynkonichiwa']['tokens'])\n",
    "\n",
    "print('\\nRobyn Lyrics Descriptive Stats:\\n')\n",
    "flattened_descriptive_stats(lyrics_df[lyrics_df['artist']=='robyn']['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "---\n",
    "**Q:** How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "**A:** If we included the stop words in exploring the \"top 5 words\" we would have nonsense words like the, a, or and which would not provide any type of insight to lyric meanings.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "**A:** I had to look up who robynkonichiwa since I am using the provided files so I didn't have any prior beliefs about the lexical diversity between her and Cher. I'm sure if I went with the artists from assingment 1 (Kelsea Ballerini and Carrie Underwood) they would have very low lexical diversity since they are both country music artists. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return (emoji.is_emoji(s))\n",
    "\n",
    "assert(emoji.is_emoji(\"‚ù§Ô∏è\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis üòÅ\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common emojis from Cher's followers:\n",
      "[('‚ù§', 79223), ('üåà', 47549), ('‚ô•', 33978), ('üè≥', 33412), ('‚ú®', 29468), ('üíô', 21379), ('üèª', 20930), ('üåä', 20223), ('‚úå', 16773), ('üíú', 16550)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Cher emoji follower info\n",
    "cher_emoji = []\n",
    "for i in twitter_df[twitter_df['artist']=='cher']['description']:\n",
    "    for token in i:\n",
    "        if emoji.is_emoji(token):\n",
    "            cher_emoji.append(token)\n",
    "            \n",
    "# counting emojis for Cher\n",
    "cher_emoji_count = Counter(cher_emoji)\n",
    "print(\"Top 10 most common emojis from Cher's followers:\")\n",
    "print(cher_emoji_count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5829e9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common emojis from Robyn's followers:\n",
      "[('‚ù§', 4783), ('üåà', 4685), ('üè≥', 3528), ('‚ô•', 3103), ('‚ú®', 2223), ('üèª', 1495), ('‚úå', 1189), ('üèº', 1139), ('‚ôÄ', 836), ('üíô', 809)]\n"
     ]
    }
   ],
   "source": [
    "# Robyn emoji follower info\n",
    "robyn_emoji = []\n",
    "for i in twitter_df[twitter_df['artist']=='robynkonichiwa']['description']:\n",
    "    for token in i:\n",
    "        if emoji.is_emoji(token):\n",
    "            robyn_emoji.append(token)\n",
    "            \n",
    "# counting emojis for Cher\n",
    "robyn_emoji_count = Counter(robyn_emoji)\n",
    "print(\"Top 10 most common emojis from Robyn's followers:\")\n",
    "print(robyn_emoji_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 hashtags from Cher's followers:\n",
      "[('BLM', 9539), ('Resist', 6044), ('BlackLivesMatter', 4681), ('resist', 3802), ('FBR', 3241), ('TheResistance', 2998), ('blacklivesmatter', 2645), ('BidenHarris', 2640), ('Resistance', 1919), ('VoteBlue', 1878)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "hashtag_regex = re.compile(r'#([a-zA-Z]+)')\n",
    "# Cher Hashtags\n",
    "# setting up lists\n",
    "cher_hashtag = []\n",
    "\n",
    "for tag in twitter_df[twitter_df['artist']=='cher']['description']:\n",
    "    hashtag = re.findall(hashtag_regex, tag)\n",
    "    cher_hashtag.extend(hashtag)\n",
    "\n",
    "# counting top 10\n",
    "cher_top_tags = Counter(cher_hashtag)\n",
    "\n",
    "# outputs\n",
    "print(\"Top 10 hashtags from Cher's followers:\")\n",
    "print(cher_top_tags.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c6663f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 hashtags from Robyn's followers\n",
      "[('BlackLivesMatter', 337), ('BLM', 306), ('blacklivesmatter', 208), ('music', 178), ('Music', 114), ('EDM', 86), ('LGBTQ', 76), ('TeamFollowBack', 59), ('blm', 56), ('travel', 51)]\n"
     ]
    }
   ],
   "source": [
    "# Robyn Hashtags\n",
    "# setting up lists\n",
    "robyn_hashtag = []\n",
    "\n",
    "for tag in twitter_df[twitter_df['artist']=='robynkonichiwa']['description']:\n",
    "    hashtag = re.findall(hashtag_regex, tag)\n",
    "    robyn_hashtag.extend(hashtag)\n",
    "\n",
    "# counting top 10\n",
    "robyn_top_tags = Counter(robyn_hashtag)\n",
    "\n",
    "# outputs\n",
    "print(\"Top 10 hashtags from Robyn's followers\")\n",
    "print(robyn_top_tags.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>song_lyrics</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>tokens_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>cher</td>\n",
       "      <td>the cruel war\\n</td>\n",
       "      <td>the cruel war the cruel war is raging sonny ha...</td>\n",
       "      <td>[the, cruel, war, the, cruel, war, is, raging,...</td>\n",
       "      <td>[cruel, war, cruel, war, raging, sonny, fight,...</td>\n",
       "      <td>[the, cruel, war]</td>\n",
       "      <td>[cruel, war]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>cher</td>\n",
       "      <td>if i could turn back time\\n</td>\n",
       "      <td>if i could turn back time if i could turn back...</td>\n",
       "      <td>[if, i, could, turn, back, time, if, i, could,...</td>\n",
       "      <td>[could, turn, back, time, could, turn, back, t...</td>\n",
       "      <td>[if, i, could, turn, back, time]</td>\n",
       "      <td>[could, turn, back, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cher</td>\n",
       "      <td>chiquitita\\n</td>\n",
       "      <td>chiquitita chiquitita tell me whats wrong your...</td>\n",
       "      <td>[chiquitita, chiquitita, tell, me, whats, wron...</td>\n",
       "      <td>[chiquitita, chiquitita, tell, whats, wrong, y...</td>\n",
       "      <td>[chiquitita]</td>\n",
       "      <td>[chiquitita]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>cher</td>\n",
       "      <td>cry myself to sleep\\n</td>\n",
       "      <td>cry myself to sleep every night i lay my head ...</td>\n",
       "      <td>[cry, myself, to, sleep, every, night, i, lay,...</td>\n",
       "      <td>[cry, sleep, every, night, lay, head, pillow, ...</td>\n",
       "      <td>[cry, myself, to, sleep]</td>\n",
       "      <td>[cry, sleep]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>robyn</td>\n",
       "      <td>dont fucking tell me what to do\\n</td>\n",
       "      <td>dont fucking tell me what to do my drinking is...</td>\n",
       "      <td>[dont, fucking, tell, me, what, to, do, my, dr...</td>\n",
       "      <td>[dont, fucking, tell, drinking, killing, drink...</td>\n",
       "      <td>[dont, fucking, tell, me, what, to, do]</td>\n",
       "      <td>[dont, fucking, tell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>youve got that something\\n</td>\n",
       "      <td>youve got that something look at me here i am ...</td>\n",
       "      <td>[youve, got, that, something, look, at, me, he...</td>\n",
       "      <td>[youve, got, something, look, im, givin, lovin...</td>\n",
       "      <td>[youve, got, that, something]</td>\n",
       "      <td>[youve, got, something]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>robyn</td>\n",
       "      <td>robotboy\\n</td>\n",
       "      <td>robotboy hey now boy where you been smashed up...</td>\n",
       "      <td>[robotboy, hey, now, boy, where, you, been, sm...</td>\n",
       "      <td>[robotboy, hey, boy, smashed, toy, lost, circu...</td>\n",
       "      <td>[robotboy]</td>\n",
       "      <td>[robotboy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>robyn</td>\n",
       "      <td>handle me\\n</td>\n",
       "      <td>handle me yeah i heard about some guy that you...</td>\n",
       "      <td>[handle, me, yeah, i, heard, about, some, guy,...</td>\n",
       "      <td>[handle, yeah, heard, guy, beat, pretty, bad, ...</td>\n",
       "      <td>[handle, me]</td>\n",
       "      <td>[handle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>robyn</td>\n",
       "      <td>tell you today\\n</td>\n",
       "      <td>tell you today tell you tell you tell you tell...</td>\n",
       "      <td>[tell, you, today, tell, you, tell, you, tell,...</td>\n",
       "      <td>[tell, today, tell, tell, tell, tell, tell, te...</td>\n",
       "      <td>[tell, you, today]</td>\n",
       "      <td>[tell, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>cher</td>\n",
       "      <td>song called children\\n</td>\n",
       "      <td>song called children children playing in a par...</td>\n",
       "      <td>[song, called, children, children, playing, in...</td>\n",
       "      <td>[song, called, children, children, playing, pa...</td>\n",
       "      <td>[song, called, children]</td>\n",
       "      <td>[song, called, children]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                         song_title  \\\n",
       "248   cher                    the cruel war\\n   \n",
       "105   cher        if i could turn back time\\n   \n",
       "34    cher                       chiquitita\\n   \n",
       "42    cher              cry myself to sleep\\n   \n",
       "343  robyn  dont fucking tell me what to do\\n   \n",
       "419  robyn         youve got that something\\n   \n",
       "396  robyn                         robotboy\\n   \n",
       "357  robyn                        handle me\\n   \n",
       "406  robyn                   tell you today\\n   \n",
       "228   cher             song called children\\n   \n",
       "\n",
       "                                           song_lyrics  \\\n",
       "248  the cruel war the cruel war is raging sonny ha...   \n",
       "105  if i could turn back time if i could turn back...   \n",
       "34   chiquitita chiquitita tell me whats wrong your...   \n",
       "42   cry myself to sleep every night i lay my head ...   \n",
       "343  dont fucking tell me what to do my drinking is...   \n",
       "419  youve got that something look at me here i am ...   \n",
       "396  robotboy hey now boy where you been smashed up...   \n",
       "357  handle me yeah i heard about some guy that you...   \n",
       "406  tell you today tell you tell you tell you tell...   \n",
       "228  song called children children playing in a par...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "248  [the, cruel, war, the, cruel, war, is, raging,...   \n",
       "105  [if, i, could, turn, back, time, if, i, could,...   \n",
       "34   [chiquitita, chiquitita, tell, me, whats, wron...   \n",
       "42   [cry, myself, to, sleep, every, night, i, lay,...   \n",
       "343  [dont, fucking, tell, me, what, to, do, my, dr...   \n",
       "419  [youve, got, that, something, look, at, me, he...   \n",
       "396  [robotboy, hey, now, boy, where, you, been, sm...   \n",
       "357  [handle, me, yeah, i, heard, about, some, guy,...   \n",
       "406  [tell, you, today, tell, you, tell, you, tell,...   \n",
       "228  [song, called, children, children, playing, in...   \n",
       "\n",
       "                                                tokens  \\\n",
       "248  [cruel, war, cruel, war, raging, sonny, fight,...   \n",
       "105  [could, turn, back, time, could, turn, back, t...   \n",
       "34   [chiquitita, chiquitita, tell, whats, wrong, y...   \n",
       "42   [cry, sleep, every, night, lay, head, pillow, ...   \n",
       "343  [dont, fucking, tell, drinking, killing, drink...   \n",
       "419  [youve, got, something, look, im, givin, lovin...   \n",
       "396  [robotboy, hey, boy, smashed, toy, lost, circu...   \n",
       "357  [handle, yeah, heard, guy, beat, pretty, bad, ...   \n",
       "406  [tell, today, tell, tell, tell, tell, tell, te...   \n",
       "228  [song, called, children, children, playing, pa...   \n",
       "\n",
       "                             tokenized_title               tokens_title  \n",
       "248                        [the, cruel, war]               [cruel, war]  \n",
       "105         [if, i, could, turn, back, time]  [could, turn, back, time]  \n",
       "34                              [chiquitita]               [chiquitita]  \n",
       "42                  [cry, myself, to, sleep]               [cry, sleep]  \n",
       "343  [dont, fucking, tell, me, what, to, do]      [dont, fucking, tell]  \n",
       "419            [youve, got, that, something]    [youve, got, something]  \n",
       "396                               [robotboy]                 [robotboy]  \n",
       "357                             [handle, me]                   [handle]  \n",
       "406                       [tell, you, today]              [tell, today]  \n",
       "228                 [song, called, children]   [song, called, children]  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "# cleaning up titles\n",
    "# remove punctuation\n",
    "lyrics_df['song_title'] = lyrics_df['song_title'].map(remove_punctuation)\n",
    "\n",
    "# remove whitespace\n",
    "#lyrics_df['song_title'] = lyrics_df['song_title'].map(split_on_whitespace)\n",
    "\n",
    "# making lowercase\n",
    "lyrics_df['song_title'] = lyrics_df['song_title'].apply(lambda x: x.lower())\n",
    "\n",
    "# tokenizing\n",
    "lyrics_df['tokenized_title'] = lyrics_df['song_title'].map(tokenization)\n",
    "\n",
    "# removing stop words\n",
    "lyrics_df['tokens_title'] = lyrics_df['tokenized_title'].map(remove_stopwords)\n",
    "\n",
    "lyrics_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11694485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>song_lyrics</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>tokens_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>robyn</td>\n",
       "      <td>we dance to the beat\\n</td>\n",
       "      <td>we dance to the beat we dance to the beat we d...</td>\n",
       "      <td>[we, dance, to, the, beat, we, dance, to, the,...</td>\n",
       "      <td>[dance, beat, dance, beat, dance, beat, dance,...</td>\n",
       "      <td>[we, dance, to, the, beat]</td>\n",
       "      <td>[dance, beat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>robyn</td>\n",
       "      <td>where did our love go\\n</td>\n",
       "      <td>where did our love go thoughts about you and m...</td>\n",
       "      <td>[where, did, our, love, go, thoughts, about, y...</td>\n",
       "      <td>[love, go, thoughts, thinkin, used, love, stro...</td>\n",
       "      <td>[where, did, our, love, go]</td>\n",
       "      <td>[love, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>robyn</td>\n",
       "      <td>whos that girl\\n</td>\n",
       "      <td>whos that girl good girls are pretty like all ...</td>\n",
       "      <td>[whos, that, girl, good, girls, are, pretty, l...</td>\n",
       "      <td>[whos, girl, good, girls, pretty, like, time, ...</td>\n",
       "      <td>[whos, that, girl]</td>\n",
       "      <td>[whos, girl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>robyn</td>\n",
       "      <td>with every heartbeat\\n</td>\n",
       "      <td>with every heartbeat maybe we could make it al...</td>\n",
       "      <td>[with, every, heartbeat, maybe, we, could, mak...</td>\n",
       "      <td>[every, heartbeat, maybe, could, make, right, ...</td>\n",
       "      <td>[with, every, heartbeat]</td>\n",
       "      <td>[every, heartbeat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>youve got that something\\n</td>\n",
       "      <td>youve got that something look at me here i am ...</td>\n",
       "      <td>[youve, got, that, something, look, at, me, he...</td>\n",
       "      <td>[youve, got, something, look, im, givin, lovin...</td>\n",
       "      <td>[youve, got, that, something]</td>\n",
       "      <td>[youve, got, something]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                  song_title  \\\n",
       "415  robyn      we dance to the beat\\n   \n",
       "416  robyn     where did our love go\\n   \n",
       "417  robyn            whos that girl\\n   \n",
       "418  robyn      with every heartbeat\\n   \n",
       "419  robyn  youve got that something\\n   \n",
       "\n",
       "                                           song_lyrics  \\\n",
       "415  we dance to the beat we dance to the beat we d...   \n",
       "416  where did our love go thoughts about you and m...   \n",
       "417  whos that girl good girls are pretty like all ...   \n",
       "418  with every heartbeat maybe we could make it al...   \n",
       "419  youve got that something look at me here i am ...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "415  [we, dance, to, the, beat, we, dance, to, the,...   \n",
       "416  [where, did, our, love, go, thoughts, about, y...   \n",
       "417  [whos, that, girl, good, girls, are, pretty, l...   \n",
       "418  [with, every, heartbeat, maybe, we, could, mak...   \n",
       "419  [youve, got, that, something, look, at, me, he...   \n",
       "\n",
       "                                                tokens  \\\n",
       "415  [dance, beat, dance, beat, dance, beat, dance,...   \n",
       "416  [love, go, thoughts, thinkin, used, love, stro...   \n",
       "417  [whos, girl, good, girls, pretty, like, time, ...   \n",
       "418  [every, heartbeat, maybe, could, make, right, ...   \n",
       "419  [youve, got, something, look, im, givin, lovin...   \n",
       "\n",
       "                   tokenized_title             tokens_title  \n",
       "415     [we, dance, to, the, beat]            [dance, beat]  \n",
       "416    [where, did, our, love, go]               [love, go]  \n",
       "417             [whos, that, girl]             [whos, girl]  \n",
       "418       [with, every, heartbeat]       [every, heartbeat]  \n",
       "419  [youve, got, that, something]  [youve, got, something]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "54d06eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five words from Cher's song titles:\n",
      "[('love', 38), ('man', 12), ('song', 11), ('dont', 10), ('come', 7)]\n",
      "Top five words from Robyn's song titles:\n",
      "[('love', 6), ('dont', 4), ('u', 4), ('thing', 3), ('girl', 3)]\n"
     ]
    }
   ],
   "source": [
    "# finding top 5 common words in song titles\n",
    "# Cher\n",
    "cher_titles = []\n",
    "for title in lyrics_df[lyrics_df['artist']=='cher']['tokens_title']:\n",
    "    for word in title:\n",
    "        cher_titles.append(word)\n",
    "cher_top_title = Counter(cher_titles)\n",
    "# outputs\n",
    "print(\"Top five words from Cher's song titles:\")\n",
    "print(cher_top_title.most_common(5))\n",
    "\n",
    "# Robyn\n",
    "robyn_titles = []\n",
    "for title in lyrics_df[lyrics_df['artist']=='robyn']['tokens_title']:\n",
    "    for word in title:\n",
    "        robyn_titles.append(word)\n",
    "robyn_top_title = Counter(robyn_titles)\n",
    "# outputs\n",
    "print(\"Top five words from Robyn's song titles:\")\n",
    "print(robyn_top_title.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "---\n",
    "**Q:** What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "**A:** whitespace character ([\\r\\n\\t\\f\\v ]) at least once.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>song_lyrics</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>tokens_title</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>cher</td>\n",
       "      <td>you dont have to say you love me\\n</td>\n",
       "      <td>you dont have to say you love me when i said i...</td>\n",
       "      <td>[you, dont, have, to, say, you, love, me, when...</td>\n",
       "      <td>[dont, say, love, said, needed, said, would, a...</td>\n",
       "      <td>[you, dont, have, to, say, you, love, me]</td>\n",
       "      <td>[dont, say, love]</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>robyn</td>\n",
       "      <td>should have known\\n</td>\n",
       "      <td>should have known i should have seen it coming...</td>\n",
       "      <td>[should, have, known, i, should, have, seen, i...</td>\n",
       "      <td>[known, seen, coming, fucking, known, could, l...</td>\n",
       "      <td>[should, have, known]</td>\n",
       "      <td>[known]</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>robyn</td>\n",
       "      <td>baby forgive me\\n</td>\n",
       "      <td>baby forgive me here come the night in your ey...</td>\n",
       "      <td>[baby, forgive, me, here, come, the, night, in...</td>\n",
       "      <td>[baby, forgive, come, night, eyes, baby, brave...</td>\n",
       "      <td>[baby, forgive, me]</td>\n",
       "      <td>[baby, forgive]</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>cher</td>\n",
       "      <td>superstar\\n</td>\n",
       "      <td>superstar long ago and so far away ah i fell i...</td>\n",
       "      <td>[superstar, long, ago, and, so, far, away, ah,...</td>\n",
       "      <td>[superstar, long, ago, far, away, ah, fell, lo...</td>\n",
       "      <td>[superstar]</td>\n",
       "      <td>[superstar]</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>after all\\n</td>\n",
       "      <td>after all well here we are again i guess it mu...</td>\n",
       "      <td>[after, all, well, here, we, are, again, i, gu...</td>\n",
       "      <td>[well, guess, must, fate, weve, tried, deep, i...</td>\n",
       "      <td>[after, all]</td>\n",
       "      <td>[]</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                          song_title  \\\n",
       "307   cher  you dont have to say you love me\\n   \n",
       "402  robyn                 should have known\\n   \n",
       "319  robyn                   baby forgive me\\n   \n",
       "238   cher                         superstar\\n   \n",
       "2     cher                         after all\\n   \n",
       "\n",
       "                                           song_lyrics  \\\n",
       "307  you dont have to say you love me when i said i...   \n",
       "402  should have known i should have seen it coming...   \n",
       "319  baby forgive me here come the night in your ey...   \n",
       "238  superstar long ago and so far away ah i fell i...   \n",
       "2    after all well here we are again i guess it mu...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "307  [you, dont, have, to, say, you, love, me, when...   \n",
       "402  [should, have, known, i, should, have, seen, i...   \n",
       "319  [baby, forgive, me, here, come, the, night, in...   \n",
       "238  [superstar, long, ago, and, so, far, away, ah,...   \n",
       "2    [after, all, well, here, we, are, again, i, gu...   \n",
       "\n",
       "                                                tokens  \\\n",
       "307  [dont, say, love, said, needed, said, would, a...   \n",
       "402  [known, seen, coming, fucking, known, could, l...   \n",
       "319  [baby, forgive, come, night, eyes, baby, brave...   \n",
       "238  [superstar, long, ago, far, away, ah, fell, lo...   \n",
       "2    [well, guess, must, fate, weve, tried, deep, i...   \n",
       "\n",
       "                               tokenized_title       tokens_title  length  \n",
       "307  [you, dont, have, to, say, you, love, me]  [dont, say, love]      87  \n",
       "402                      [should, have, known]            [known]      97  \n",
       "319                        [baby, forgive, me]    [baby, forgive]      77  \n",
       "238                                [superstar]        [superstar]      93  \n",
       "2                                 [after, all]                 []     120  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "# making a copy, just in case... tired of scrolling back\n",
    "lyrics_end_df = lyrics_df\n",
    "\n",
    "# adding length column\n",
    "lyrics_end_df['length'] = [len(lyrics_end_df['tokens'][i]) for i in range(len(lyrics_end_df))]\n",
    "lyrics_end_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "22404be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "cher     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "robyn    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "    # df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)\n",
    "lyrics_end_df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
